{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os as os\n",
    "\n",
    "# make plots pretty\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', color_codes=True)\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# scores & validation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from matplotlib.ticker import StrMethodFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1 - Data Import\n",
    "Before we can even begin to work, we are required to load data into a pandas dataframe.  In our particular case, we have data stored in csv file and can utilize the Pandas read_csv function.\n",
    "\n",
    "The Pandas documentation outlines various other ways to import data: https://pandas.pydata.org/pandas-docs/stable/api.html#input-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) With the data imported, print out the first 5 rows of data.  \n",
    "* Do you notice anything wrong with the data? \n",
    "* How can we correct this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Print out the last 5 rows of data. \n",
    "* Do you notice anything wrong? \n",
    "* How can we correct this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Exploratory Data Analysis (EDA)\n",
    "Next, we get to know the data that we are working with.  EDAs can be exhaustive - We want to gather insights on our dataset, identify trends, discovery data anomalies, etc.\n",
    "\n",
    "For our workshop, we will do a quick dive into several key fields to try and gain an understanding of the loans we are working with.  NOTE: This is not an exhaustive EDA.  There are plenty of deep dives that can be done on the variables present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Print a summary of the dataframe\n",
    "* Is this useful?\n",
    "* How do we correct this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Print the descriptive statistics for the column named 'loan_amnt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing unneccessary data\n",
    "As noted by the above summary, we have a ton of data columns within this dataset - many of which contain missing data and others which will not provide any predictive power for our model. \n",
    "\n",
    "In order to simplify this workshop, we will drop a large number of data columns.  The list of columns is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['id', 'emp_title','desc', 'mths_since_last_delinq', 'mths_since_last_record','next_pymnt_d',\n",
    "               'settlement_status', 'settlement_date',  'settlement_amount', 'settlement_percentage', \n",
    "                'settlement_term', 'debt_settlement_flag_date', 'pub_rec_bankruptcies', 'application_type',\n",
    "                'annual_inc', 'verification_status', 'debt_settlement_flag', 'disbursement_method', \n",
    "                'hardship_flag', 'initial_list_status', 'pymnt_plan', 'zip_code', 'earliest_cr_line', 'title',\n",
    "                'revol_util', 'last_pymnt_d', 'last_credit_pull_d', 'total_pymnt', 'total_pymnt_inv', 'total_rec_int',\n",
    "                'total_rec_late_fee', 'total_rec_prncp', 'funded_amnt', 'funded_amnt_inv', 'out_prncp', 'out_prncp_inv', \n",
    "                'last_pymnt_amnt',  'tax_liens', 'delinq_amnt', 'policy_code', 'collections_12_mths_ex_med', \n",
    "                'chargeoff_within_12_mths']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Drop unneccessary data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Drop columns containing all null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns containing null data\n",
    "# - axis=1 indicates we want to drop columns\n",
    "# - how=all indicates that we will only drop a column if ALL values are null\n",
    "# - inplace indicates that we will remove columns from the listed dataframe rather than returning a copy\n",
    "df.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Drop any row containing null data\n",
    "In a normal circumstance, we would take more care into retaining as much data as possible for our model.  In this workshop where we are constrained on time, we will ruthlessly drop any row containing null data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop data\n",
    "# - axis=0 means we will drop rows\n",
    "# - how=any means we will drop rows containing ANY null data\n",
    "df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Display a summary of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe summary\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Data Types\n",
    "For the most part, Machine Learning models are only capable of handling numeric values.  We must prepare our data to ensure columns are correctly defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates\n",
    "Dates cannot be interpreted by machine learning models.  However, we can take components of a date to be used in a model.  In this section we will process our issue_d column and capture information related to the issue date of a loan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g) Convert issue_d into a datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_d'] = pd.to_datetime(df.issue_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h) Create two new features to capture the issue date 'year' and 'month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_d_year'] = df.issue_d.dt.year\n",
    "df['issue_d_month'] = df.issue_d.dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical vs Numeric\n",
    "When creating your model, you will often be faced with the challenge of deciding whether or not a value is categorical or numeric.  For example - is age a categorical value or numeric value?  Is the month in which a loan is issued a categorical value or numeric value? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Convert the issue date year and month into a categorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_d_year'] = pd.Categorical(df.issue_d_year)\n",
    "df['issue_d_month'] = pd.Categorical(df.issue_d_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### j) Convert int_rate into a numerical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['int_rate'] = [pd.to_numeric(i[:-1]) for i in df.int_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k) Covert the remaining 'object' columns into categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert each object column into categorical \n",
    "for c in df.select_dtypes(include=['object']):\n",
    "    df[c] = pd.Categorical(df[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Default Loans\n",
    "The current dataset does not provide a column that identifies whether or not a loan has defaulted.  A loan status column provides us insight into the current stats of a loan.  Looking further at the documentation online, we find that there are specific statuses that identify loans in good health.\n",
    "\n",
    "Using these 'good health' statuses, we will create a feature that identifies whether or not a loan has defaulted, or expected to go into default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Good loan status\n",
    "good_status = [\n",
    "    'Current',\n",
    "    'Fully Paid',\n",
    "    'Issued',\n",
    "    'Does not meet the credit policy. Status:Fully Paid',\n",
    "    'In Grace Period'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### l) Create a new field named 'is_default' that identifies whether or not a loan is in default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_default'] = [0 if s in good_status else 1 for s in df.loan_status]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### j) What is the breakdown of loans (default vs non-default)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.is_default.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets take a look at a breakdown of default vs non-default laons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine data\n",
    "default = df[df.is_default==1].loan_amnt\n",
    "non_default = df[df.is_default==0].loan_amnt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# create plots\n",
    "ax.hist(non_default, bins=7, stacked=True, label='non-default')\n",
    "ax.hist(default, bins=7, stacked=True, label='default')\n",
    "\n",
    "# update titles\n",
    "ax.set_title('Default vs Non-Default Loans')\n",
    "ax.set_xlabel('Loan Amount ($)')\n",
    "\n",
    "# setup labels\n",
    "ax.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# set legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Loan Interest Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup bins\n",
    "bins = np.arange(0, 35, 2.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "# create the histogram\n",
    "values = df.int_rate\n",
    "ax.hist(values, bins=bins, edgecolor='lightgrey')\n",
    "\n",
    "# setup labels\n",
    "ax.set_xlabel('Interest Rates (%)')\n",
    "ax.xaxis.set_major_formatter(StrMethodFormatter('{x:,.1f}'))\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of grades\n",
    "grades = df.grade.unique()\n",
    "\n",
    "# create a colormap to represent each grade\n",
    "cm = plt.cm.rainbow\n",
    "colors = cm(np.linspace(0, 1, len(grades)))\n",
    "\n",
    "# Create a figure with nrows= total number of grades\n",
    "fig, axs = plt.subplots(len(grades), sharey=True, figsize=(14,18))\n",
    "\n",
    "# for each grade, create a graph\n",
    "for i, g in enumerate(sorted(grades)):\n",
    "    line = axs[i].hist(df[df.grade==g].int_rate, bins=bins, color=colors[i], label=g, edgecolor='lightblue')\n",
    "    axs[i].set_xlabel('Interest Rate (%)')\n",
    "    axs[i].legend()\n",
    "\n",
    "# Set labels\n",
    "axs[0].set_title('Interest Frequency by Grade')\n",
    "\n",
    "# adjust spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Loans by issue_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of grades\n",
    "grades = df.grade.unique()\n",
    "\n",
    "# create a colormap to represent each grade\n",
    "cm = plt.cm.rainbow\n",
    "colors = cm(np.linspace(0, 1, len(grades)))\n",
    "\n",
    "# create plot\n",
    "df.groupby(['issue_d', 'grade']).loan_amnt.sum().unstack().plot.area(figsize=(14,6), color=colors)\n",
    "\n",
    "# set labels\n",
    "plt.title('Loans Issued by Date & Grade')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.xlabel('Date Issued (Month/Year)')\n",
    "\n",
    "# get current axis\n",
    "ax = plt.gca()\n",
    "\n",
    "# disable scientific notation on the y axis\n",
    "ax.ticklabel_format(axis='y', style='plain')\n",
    "\n",
    "# format values with comma\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Loan Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we know what types of loans have the best chance of being repaid? \n",
    "loans = []\n",
    "\n",
    "for p in df.purpose.unique():\n",
    "    good = np.sum([(df.purpose==p) & (df.is_default==0)]) / np.sum(df.purpose==p)\n",
    "    bad = np.sum([(df.purpose==p) & (df.is_default==1)]) / np.sum(df.purpose==p)\n",
    "\n",
    "    loans.append([p, good, bad])\n",
    "\n",
    "loans = pd.DataFrame(loans)\n",
    "loans.columns = ['Purpose', 'Non-default', 'Default']\n",
    "loans.set_index('Purpose', inplace=True)\n",
    "loans.sort_values('Non-default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Loan Amounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k) Create a histogram for the loan_amnts columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "# setup plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# create graph\n",
    "values = df.loan_amnt\n",
    "ax.hist(values, bins=7, edgecolor='lightblue')\n",
    "\n",
    "# setup titles\n",
    "ax.set_title('Loan Distribution')\n",
    "ax.set_xlabel('Loan Amount ($)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "# format ticks\n",
    "ax.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matix\n",
    "A correlation matix allows you to see how well the features in your model are correlated to your target variable.  The better the correlation, the more predictive power that feature contains. \n",
    "\n",
    "It also allows you to see features that might be correlated to one another and therefore should be removed from your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "f, ax = plt.subplots(figsize=(14, 14))\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax, annot=True, fmt='.1f')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Multicollinearity\n",
    "In order to ensure we have a stable model, I will remove total_acc & collection_recovery_fee, which appear to have high correlation with existing variables in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_corr_feat = ['total_acc', 'collection_recovery_fee']\n",
    "\n",
    "df.drop(labels=drop_corr_feat, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Model Building\n",
    "We've done a fair amout of data prep already - our next step is to begin modeling building.\n",
    "\n",
    "Our goal for this classification exercise is to create a model that predicts whether or not a loanee will default.  In order for this tomwork, we need to build a model using only completed loans (meaning loans that have been fully repaid or defaulted). \n",
    "\n",
    "We will filter our dataset to remove any loans that have not been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### l) Remove loans which are not in a completed_loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_loan_status = ['Fully Paid', \n",
    "                         'Charged Off', \n",
    "                         'Default',\n",
    "                         'Late (16-30 days)',\n",
    "                         'Late (31-120 days)'\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = df[df.loan_status.isin(completed_loan_status)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our current dataset, we have several columns which are heavily correlated to the status of a loan.  We need to remove these values from our training data as we want to ensure we can predict default status based on data available at the time of the loan. \n",
    "\n",
    "Note: we will also include 'issue_d' within this list - as it is a datetime column and not required for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### j) Remove the following columns from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['grade', 'sub_grade', 'issue_d', 'loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.drop(labels=cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Data (X, y)\n",
    "We need to split our data into two datasets representing the features of the model (predictors) and the target (outcome).  \n",
    "\n",
    "Remember that our machine learning model is only able to consume numeric values.  We need to convert categorical values into numbers prior to training our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k) Define our X & y datasets\n",
    "* How do the columns change after converting categorical values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining model data\n",
    "X = pd.get_dummies(model_data.drop(labels=['is_default'], axis=1))\n",
    "y = model_data.is_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "It is important to split our data into training & testing datasets.  This allows us to validate our model using data it has never seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### l) Split our X & Y data into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data\n",
    "Scaling the data ensures that values are compared on the same scale - for example, it would be hard to compare annual income and age.  Scaling the data allows us to bring these values down to a simliar scale in which the machine learning algorithm is able to weight the values approparitely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m) Define a StandardScaler.  Scale the X_train & X_test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# fit & transform the training data\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "\n",
    "# we only want to transform the text data\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Classifier\n",
    "Our data is ready to go.  The next step is to create a classifier.  For our example, we will utilize Logistic Regression.  \n",
    "\n",
    "Note: It is important to evaluate the performance of multiple models as models will behave differently based on the data provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n) Define a LogisticRegression classifier.  Fit the classifier with the appropriate training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the classifier\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "With the model fitted, we can predict results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### o) predict results for both the training & testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the test dataset\n",
    "y_pred_lr = clf_lr.predict(X_train_scaled)\n",
    "y_pred_lr_test = clf_lr.predict(X_test_scaled)\n",
    "\n",
    "# find the prediction precentage for the 'default' class (which is the second column)\n",
    "y_score_lr = clf_lr.predict_proba(X_train_scaled)[:,1]\n",
    "y_score_lr_test = clf_lr.predict_proba(X_test_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score The Results\n",
    "Lets examine our predictions results in order to determine how our model is performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the scores\n",
    "accuracy_score = clf_lr.score(X_train_scaled, y_train)\n",
    "roc_auc = roc_auc_score(y_train, y_score_lr)\n",
    "\n",
    "accuracy_score_test = clf_lr.score(X_test_scaled, y_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_score_lr_test)\n",
    "\n",
    "# print scores\n",
    "print('Training Scores')\n",
    "print('Accuracy: {}'.format(accuracy_score))\n",
    "print('ROC AUC: {}'.format(roc_auc))\n",
    "print()\n",
    "print('Testing Scores')\n",
    "print('Accuracy: {}'.format(accuracy_score_test))\n",
    "print('ROC AUC: {}'.format(roc_auc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the classification report for our test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lr_test, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Scoring Metrics\n",
    "Looking at the accuracy & ROC AUC scores, we would think our model is performing well.  However, the classification report tells otherwise.  We are struggling to correctly classify defaulted loans.  How do we correct this? \n",
    "\n",
    "Let's try balancing our dataset and trying again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - SMOTE Class Balancing\n",
    "Using SMOTE, we are able to bring balance to our minority class, which will hopefully improve our accuracy score.\n",
    "\n",
    "SMOTE introduces sythetic data.  It is important to ensure that sythetic data is only introduced into our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# display the sampled classes\n",
    "print('Class distribution prior to SMOTE {}'.format(Counter(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# balance our class\n",
    "sm = SMOTE(ratio='minority', random_state=42)\n",
    "X_res, y_res = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "# display the sampled classes\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain & score our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale & transform the data\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_res)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "# fit the classifier\n",
    "clf_lr_balanced = LogisticRegression()\n",
    "clf_lr_balanced.fit(X_train_scaled, y_res)\n",
    "\n",
    "# predict using the test dataset\n",
    "y_pred_lr = clf_lr_balanced.predict(X_train_scaled)\n",
    "y_pred_lr_test = clf_lr_balanced.predict(X_test_scaled)\n",
    "\n",
    "# find the prediction precentage for the 'default' class (which is the second column)\n",
    "y_score_lr = clf_lr_balanced.predict_proba(X_train_scaled)[:,1]\n",
    "y_score_lr_test = clf_lr_balanced.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "# find the scores\n",
    "accuracy_score = clf_lr_balanced.score(X_train_scaled, y_res)\n",
    "roc_auc = roc_auc_score(y_res, y_score_lr)\n",
    "\n",
    "accuracy_score_test = clf_lr_balanced.score(X_test_scaled, y_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_score_lr_test)\n",
    "\n",
    "# print scores\n",
    "print('Training Scores')\n",
    "print('Accuracy: {}'.format(accuracy_score))\n",
    "print('ROC AUC: {}'.format(roc_auc))\n",
    "print()\n",
    "print('Testing Scores')\n",
    "print('Accuracy: {}'.format(accuracy_score_test))\n",
    "print('ROC AUC: {}'.format(roc_auc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lr_test, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your specific use case, you may want to tune your parameters to achieve better results in precision or recall.\n",
    "\n",
    "In our specific example, is it better to \n",
    "* Provide a loan to somone who will default\n",
    "* Deny a loan to someone who will not defaul\n",
    "\n",
    "These are the questions you need to validate with your stakeholders and take into account when building your model.\n",
    "\n",
    "**Bias is also a very big factor that needs to be tested**\n",
    "* Is your model biased based on an applicants state? \n",
    "* Does your model favor a particular age range? \n",
    "* etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Feature Importances\n",
    "Certain algorithms have the ability to print feature importances.  This allows you to see which features have strong predictive power, and conversely, which features do not have the predictive power you thought they had. \n",
    "\n",
    "For logistic regression, feature importances can be access from the .coef_[0] attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted(list(zip(list(X.columns), list(clf_lr_balanced.coef_[0]))), key=itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Pipelines\n",
    "Pipelines allow us to streamline model building.  They allow us to string together multiple transformation steps, which ultimately feed into an estimator.  A bonus to pipelines is that they allow us to quickly swap transformations and estimators as needed.  \n",
    "\n",
    "Each step is defined with a tuple that defines the step name and object to be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# fit the data\n",
    "pipe_lr.fit(X_res, y_res)\n",
    "\n",
    "# predict using the test dataset\n",
    "y_pred_lr = pipe_lr.predict(X_res)\n",
    "y_pred_lr_test = pipe_lr.predict(X_test)\n",
    "\n",
    "# find the prediction precentage for the 'default' class (which is the second column)\n",
    "y_score_lr = pipe_lr.predict_proba(X_res)[:,1]\n",
    "y_score_lr_test = pipe_lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "# find the scores\n",
    "accuracy_score = pipe_lr.score(X_res, y_res)\n",
    "roc_auc = roc_auc_score(y_res, y_score_lr)\n",
    "\n",
    "accuracy_score_test = pipe_lr.score(X_test, y_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_score_lr_test)\n",
    "\n",
    "# print scores\n",
    "print('Training Scores')\n",
    "print('Accuracy: {}'.format(accuracy_score))\n",
    "print('ROC AUC: {}'.format(roc_auc))\n",
    "print()\n",
    "print('Testing Scores')\n",
    "print('Accuracy: {}'.format(accuracy_score_test))\n",
    "print('ROC AUC: {}'.format(roc_auc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the pipeline has removed the need to scale data prior to feeding it into the pipeline.  This is because the pipeline knows how to appropriate apply scaling to incoming data.\n",
    "\n",
    "This become powerful as you begin to add dimensionality reduction and other transformations to your pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - Hyperparameter Tuning\n",
    "Use hyperparameter tuning to improve model performance by selecting optimal parameters for your data.\n",
    "\n",
    "The param_grid identifies the parameters and values which we want to test.  \n",
    "\n",
    "In our example below, we will utilize GridSearchCV to search for the best parameters.  Note that GridSearchCV will test all possible combinations of parameters.  In cases where you have an large number of paramters - this can take an extrodinary amount of time.  Other options such as RandomizedSearchCV are available to test smaller combinations when sensitive on time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will do hyperparameter tuning on our logisitic regression\n",
    "param_grid = [{'clf__C':[1, 10, 100, 250, 500, 750, 1000]}]\n",
    "\n",
    "# Build pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# create our grid search\n",
    "grid_search = GridSearchCV(pipe_lr, param_grid, n_jobs=-1, scoring='roc_auc', return_train_score=False)\n",
    "\n",
    "# perform the fit\n",
    "grid_search.fit(X_train_scaled, y_res)\n",
    "\n",
    "# print out our best params\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(C=10))\n",
    "])\n",
    "\n",
    "# fit the data\n",
    "pipe_lr.fit(X_res, y_res)\n",
    "\n",
    "# predict using the test dataset\n",
    "y_pred_lr = pipe_lr.predict(X_res)\n",
    "y_pred_lr_test = pipe_lr.predict(X_test)\n",
    "\n",
    "# find the prediction precentage for the 'default' class (which is the second column)\n",
    "y_score_lr = pipe_lr.predict_proba(X_res)[:,1]\n",
    "y_score_lr_test = pipe_lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "# find the scores\n",
    "accuracy_score = pipe_lr.score(X_res, y_res)\n",
    "roc_auc = roc_auc_score(y_res, y_score_lr)\n",
    "\n",
    "accuracy_score_test = pipe_lr.score(X_test, y_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_score_lr_test)\n",
    "\n",
    "# print scores\n",
    "print('Training Scores')\n",
    "print('Accuracy: {}'.format(accuracy_score))\n",
    "print('ROC AUC: {}'.format(roc_auc))\n",
    "print()\n",
    "print('Testing Scores')\n",
    "print('Accuracy: {}'.format(accuracy_score_test))\n",
    "print('ROC AUC: {}'.format(roc_auc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lr_test, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 - Change to XGBClassifier\n",
    "In order to show you the versatility of pipeline, let's go ahead a swap out LogisticRegression for XGBClassifier. I will not touch any of the other code.  When we run this, what are the results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', XGBClassifier())\n",
    "])\n",
    "\n",
    "# fit the data\n",
    "pipe_lr.fit(X_res, y_res)\n",
    "\n",
    "# predict using the test dataset\n",
    "y_pred_lr = pipe_lr.predict(X_res)\n",
    "y_pred_lr_test = pipe_lr.predict(X_test)\n",
    "\n",
    "# find the prediction precentage for the 'default' class (which is the second column)\n",
    "y_score_lr = pipe_lr.predict_proba(X_res)[:,1]\n",
    "y_score_lr_test = pipe_lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "# find the scores\n",
    "accuracy_score = pipe_lr.score(X_res, y_res)\n",
    "roc_auc = roc_auc_score(y_res, y_score_lr)\n",
    "\n",
    "accuracy_score_test = pipe_lr.score(X_test, y_test)\n",
    "roc_auc_test = roc_auc_score(y_test, y_score_lr_test)\n",
    "\n",
    "# print scores\n",
    "print('Training Scores')\n",
    "print('Accuracy: {}'.format(accuracy_score))\n",
    "print('ROC AUC: {}'.format(roc_auc))\n",
    "print()\n",
    "print('Testing Scores')\n",
    "print('Accuracy: {}'.format(accuracy_score_test))\n",
    "print('ROC AUC: {}'.format(roc_auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lr_test, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional topics to discuss\n",
    "* Cross Validation\n",
    "* Feature Engineering\n",
    "* Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
